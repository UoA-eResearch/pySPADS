{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "from PyEMD import CEEMDAN, Visualisation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d83509c91dcc573",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Notes\n",
    "The final program will likely be a CLI with arguments, some of the options should be:\n",
    "- Input data\n",
    "- Output data\n",
    "- Whether to interpolate over missing data (initially linear)\n",
    "- Whether to reject components which are primarily noise (with note that this may not be valid with CEEMDAN)\n",
    "    - Apriori or aposteriori test? \n",
    "- Tolerance when matching components by frequency\n",
    "- Whether to allow input components to be used in multiple output components\n",
    "- Time range for training/testing/prediction data\n",
    "- Noise values"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4cb5405cb41ca032"
  },
  {
   "cell_type": "code",
   "source": [
    "# Load data\n",
    "# TODO - note that we're not interpolating over gaps here\n",
    "from processing.bridge import load_shorecast, load_hindcast, load_SLP\n",
    "\n",
    "shore_df = load_shorecast()\n",
    "hind_df = load_hindcast()\n",
    "pca_df = load_SLP()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7c05347a7ee0e7b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Interpolate over NaNs in data\n",
    "def interpolate(df):\n",
    "    trange = list(range(df.index.min(), df.index.max() + 1))\n",
    "    return (\n",
    "        df\n",
    "        .reindex(trange, fill_value=np.nan)\n",
    "        .interpolate(method='linear')\n",
    "    )\n",
    "\n",
    "shore_df = interpolate(shore_df)\n",
    "hind_df = interpolate(hind_df)\n",
    "pca_df = interpolate(pca_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68b977d617debba5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from visualisation import paper\n",
    "\n",
    "f = paper.fig1(pca_df['PC0'], hind_df['Hs'], hind_df['Tp'], hind_df['Dir'], '2004-01-01', '2016-01-01')\n",
    "\n",
    "f.savefig('fig1.png')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca2cf39368d7c1b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Set up a list of each timeseries to decompose\n",
    "# Each item in the list is a tuple of:\n",
    "#   label, source df, source column\n",
    "series = [\n",
    "    ('output', shore_df, 'y'),\n",
    "] + [\n",
    "    (l, hind_df, l) for l in hind_df.columns\n",
    "] + [\n",
    "    (l, pca_df, l) for l in pca_df.columns\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "921cf5aa5a0138f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Set up CEEMD driver\n",
    "NR = 100\n",
    "ns = np.arange(0.1, 0.5, .05)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1c80b31ee5b5254",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Individually decompose each signal\n",
    "folder = 'output/imfs'\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "for label, df, col in series:\n",
    "    file = f'{folder}/{label}_{25}.csv'\n",
    "    if os.path.exists(file):\n",
    "        print(f'Skipping {label}')\n",
    "    else:\n",
    "        print(f'Decomposing {label}')\n",
    "        ceemd = CEEMDAN(trails=NR, epsilon=0.2, processes=8, parallel=True)\n",
    "        imfs = ceemd.ceemdan(df[col].to_numpy(), df.index.to_numpy(), progress=True)\n",
    "        imfs_df = pd.DataFrame(imfs.T, index=df.index)\n",
    "        imfs_df.to_csv(file)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "466a5f6c8a74d8d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Load all the IMFs\n",
    "folder = 'output/imfs'\n",
    "all_imfs = {}\n",
    "for label, _, _ in series:\n",
    "    file = f'{folder}/{label}_{25}.csv'\n",
    "    all_imfs[label] = pd.read_csv(file, index_col=0)\n",
    "\n",
    "    # Column names are strings, convert to ints\n",
    "    all_imfs[label].columns = all_imfs[label].columns.astype(int)    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a9ce0031f16e16d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Plot the IMFs\n",
    "from visualisation.imfs import plot_imfs\n",
    "\n",
    "folder = 'output/plots/imfs'\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "for label, imf_df in tqdm(all_imfs.items(), desc='Plotting IMFs'):\n",
    "    plot_imfs(imf_df.to_numpy().T, label, f'{folder}/{label}.png')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b2b23c2b2505167",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "## Reject components which are primarily noise\n",
    "# Note that this might not be valid when applied to the result of CEEMDAN (rather than EMD), so it should be optional\n",
    "from PyEMD.checks import whitenoise_check\n",
    "for label, imf_df in all_imfs.items():\n",
    "    print(f'Checking {label}')\n",
    "    sig = whitenoise_check(imf_df.to_numpy().T) #, test_name='apriori')\n",
    "    rejects = [k for k, v in sig.items() if v == 0]\n",
    "    print(f'Rejecting: {rejects}')\n",
    "    all_imfs[label] = imf_df.drop(columns=[i-1 for i in rejects])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9c2bf1a6e65450a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Number of components in each signal\n",
    "print('Number of component IMFs in each signal:')\n",
    "for label, imf_df in all_imfs.items():\n",
    "    print(f'{label:>12}: {imf_df.shape[1]}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d066e976c308a1e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "## Match up components in input/output by frequency\n",
    "from processing.significance import maxima\n",
    "from processing.recomposition import component_frequencies\n",
    "\n",
    "# Get the maxima of each component\n",
    "max_imfs = max([imf_df.shape[1] for imf_df in all_imfs.values()])\n",
    "freq_df = pd.DataFrame(columns=all_imfs.keys(), index=list(range(max_imfs)))\n",
    "for label, imf_df in all_imfs.items():\n",
    "    freq_df[label] = component_frequencies(imf_df)\n",
    "\n",
    "freq_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2db8d9ef34422366",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "fig2 = paper.fig2(shore_df['y'], all_imfs['output'], '1999-01-01', '2017-01-01')\n",
    "fig2.savefig('fig2.png')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8926ead5ed15f9ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "fig3 = paper.fig3(all_imfs, 'output', '1999-01-01', '2017-01-01')\n",
    "fig3.savefig('fig3.png')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21bdef4be19352a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "\n",
    "input_cols = set(freq_df.columns) - {'output'}\n",
    "output_index = all_imfs['output'].columns.tolist()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e005c55de5a6f534",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Find the closest match for each component in output\n",
    "from processing.recomposition import nearest_frequency\n",
    "\n",
    "nearest_freq = nearest_frequency(freq_df['output'], freq_df.drop(columns=['output']))\n",
    "\n",
    "nearest_freq"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19052c290966f7da",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Check to see if difference in frequencyes is within tolerance\n",
    "from processing.recomposition import frequency_difference\n",
    "\n",
    "diff_df = frequency_difference(freq_df['output'], freq_df.drop(columns=['output']), nearest_freq)\n",
    "diff_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2175a2e3d41149",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from processing.recomposition import relative_frequency_difference\n",
    "\n",
    "rel_diff_df = relative_frequency_difference(freq_df['output'], freq_df.drop(columns=['output']), nearest_freq)\n",
    "rel_diff_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5a56d017e283b67",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "tolerance = 0.25"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5ad5cfe1b979d10",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Check that each output component has some valid inputs\n",
    "valid_components = (rel_diff_df < tolerance).sum(axis=1)\n",
    "if any(valid_components == 0):\n",
    "    raise ValueError(f'No valid input components for output components: {valid_components[valid_components == 0].index}')\n",
    "\n",
    "if any(valid_components < 3):\n",
    "    print(f'Warning: some output components have less than 3 valid input components: {valid_components[valid_components < 3].index}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eda87c917df7bef8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "rel_diff_df < tolerance"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32d4ce6ee9dddfe9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Show the components which are used for each output component\n",
    "print('Components used for each output component:')\n",
    "for i in output_index:\n",
    "    print(f'{i:>5} : {np.sum(rel_diff_df.loc[i] < tolerance)}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0161ebb7763869b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO - set nearest_freq to NaN for components which are not used"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e87c41e139abe06",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO - note below - we have more output data than input, we should have truncated the output data before fitting!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "117e6cc1efb26c34",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "## Linear regression of components to output signal\n",
    "# Note that this is much simpler if the original data interpolated over NaNs\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from optimization import mreg2, calc_SSE_all_Coef\n",
    "\n",
    "# TODO - note below - we have more output data than input, we should have truncated the output data before fitting!\n",
    "hindcast_index = all_imfs['output'].index[all_imfs['output'].index.isin(all_imfs['PC0'].index)]\n",
    "\n",
    "imf_predictions = pd.DataFrame(index=hindcast_index, columns=output_index)\n",
    "mreg_predictions = pd.DataFrame(index=hindcast_index, columns=output_index)\n",
    "\n",
    "for i, imf in enumerate(output_index):\n",
    "    print(f'Fitting component {imf}')\n",
    "    X = pd.DataFrame(index=hindcast_index)\n",
    "    for label in input_cols:\n",
    "        if rel_diff_df.loc[imf, label] < tolerance:\n",
    "            # X[label] = all_imfs[label].iloc[:, nearest_freq.loc[imf, label]]\n",
    "            X[label] = all_imfs[label].loc[all_imfs[label].index.isin(hindcast_index), nearest_freq.loc[imf, label]]\n",
    "    y = all_imfs['output'].loc[hindcast_index, imf]\n",
    "    reg = LinearRegression().fit(X, y)\n",
    "    print(f'R^2: {reg.score(X, y)}')\n",
    "    print(f'Coefficients: {reg.coef_}')\n",
    "    coefs = mreg2(y,X)\n",
    "    print(coefs)\n",
    "    print(f'Intercept: {reg.intercept_}')\n",
    "    # Plot the fit\n",
    "    plt.figure()\n",
    "    plt.plot(y, label='output')\n",
    "    plt.plot(y.index, reg.predict(X), label='fit')\n",
    "    mreg_pred = np.sum([X[c] * coefs[i] for i, c in enumerate(X.columns)], axis=0)\n",
    "    plt.plot(y.index, mreg_pred, label='mreg')\n",
    "    imf_predictions.loc[:, imf] = reg.predict(X)\n",
    "    mreg_predictions.loc[:, imf] = mreg_pred\n",
    "    plt.legend()\n",
    "    plt.title(f'Component {imf}')\n",
    "    plt.show()\n",
    "    print('\\n\\n')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "906b5b86fcec2be",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "reg.coef_"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54a5a52828f42dc8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "mreg_pred = np.sum([X[c] * coefs[i] for i, c in enumerate(X.columns)], axis=0)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d57ff4a2ed070af",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "all_imfs['output'].sum(axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a13e72dba8563a0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "imf_predictions.sum(axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46463a5715cbc599",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "fig4 = paper.fig4(shore_df['y'], imf_predictions.sum(axis=1), '2010-01-01', '2017-01-01')\n",
    "\n",
    "fig4.savefig('fig4.png')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7054df7f1d5dc84a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "fig4a = paper.fig4(shore_df['y'], mreg_predictions.sum(axis=1), '2010-01-01', '2017-01-01')\n",
    "\n",
    "fig4a.savefig('fig4a.png')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ba2f7d352b0cbee",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from processing.bridge import datenum_to_datetime\n",
    "# TODO - note that above covers 1998-, paper uses only 2010- data\n",
    "datenum_to_datetime(737000)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ebb310c682f4776d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO: \n",
    "#   repeat the process for other noise values (and average?)\n",
    "#   tweak tolerance, noises, other settings?\n",
    "#   check regression implementation\n",
    "#   "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a02894060b6a5115",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
