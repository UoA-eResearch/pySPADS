{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d83509c91dcc573",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from PyEMD import CEEMDAN\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb5405cb41ca032",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Notes\n",
    "The final program will likely be a CLI with arguments, some of the options should be:\n",
    "- Input data\n",
    "- Output data\n",
    "- Whether to interpolate over missing data (initially linear)\n",
    "- Whether to reject components which are primarily noise (with note that this may not be valid with CEEMDAN)\n",
    "    - Apriori or aposteriori test? \n",
    "- Tolerance when matching components by frequency\n",
    "- Whether to allow input components to be used in multiple output components\n",
    "- Time range for training/testing/prediction data\n",
    "- Noise values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6545c838a197231",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = Path(\"data\") / \"run_notebook\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c05347a7ee0e7b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "# TODO - note that we're not interpolating over gaps here\n",
    "from pySPADS.processing.bridge import load_shorecast, load_hindcast, load_SLP\n",
    "\n",
    "shore_df = load_shorecast()\n",
    "hind_df = load_hindcast()\n",
    "pca_df = load_SLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b977d617debba5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Interpolate over NaNs in data\n",
    "def interpolate(df):\n",
    "    trange = list(range(df.index.min(), df.index.max() + 1))\n",
    "    return df.reindex(trange, fill_value=np.nan).interpolate(method=\"linear\")\n",
    "\n",
    "\n",
    "shore_df = interpolate(shore_df)\n",
    "hind_df = interpolate(hind_df)\n",
    "pca_df = interpolate(pca_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2cf39368d7c1b9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pySPADS.visualisation import paper\n",
    "\n",
    "f = paper.fig1(\n",
    "    pca_df[\"PC0\"],\n",
    "    hind_df[\"Hs\"],\n",
    "    hind_df[\"Tp\"],\n",
    "    hind_df[\"Dir\"],\n",
    "    \"2004-01-01\",\n",
    "    \"2016-01-01\",\n",
    ")\n",
    "\n",
    "f.savefig(folder / \"figures\" / \"fig1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921cf5aa5a0138f2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up a list of each timeseries to decompose\n",
    "# Each item in the list is a tuple of:\n",
    "#   label, source df, source column\n",
    "series = (\n",
    "    [\n",
    "        (\"output\", shore_df, \"y\"),\n",
    "    ]\n",
    "    + [(label, hind_df, label) for label in hind_df.columns]\n",
    "    + [(label, pca_df, label) for label in pca_df.columns]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c80b31ee5b5254",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up CEEMD driver\n",
    "NR = 100\n",
    "ns = np.arange(0.1, 0.5, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466a5f6c8a74d8d9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Individually decompose each signal\n",
    "imf_folder = folder / \"imfs\"\n",
    "os.makedirs(imf_folder, exist_ok=True)\n",
    "\n",
    "for label, df, col in series:\n",
    "    file = f\"{imf_folder}/{label}_{25}.csv\"\n",
    "    if os.path.exists(file):\n",
    "        print(f\"Skipping {label}\")\n",
    "    else:\n",
    "        print(f\"Decomposing {label}\")\n",
    "        ceemd = CEEMDAN(trails=NR, epsilon=0.2, processes=8, parallel=True)\n",
    "        imfs = ceemd.ceemdan(df[col].to_numpy(), df.index.to_numpy(), progress=True)\n",
    "        imfs_df = pd.DataFrame(imfs.T, index=df.index)\n",
    "        imfs_df.to_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9ce0031f16e16d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load all the IMFs\n",
    "all_imfs = {}\n",
    "for label, _, _ in series:\n",
    "    file = f\"{imf_folder}/{label}_{25}.csv\"\n",
    "    all_imfs[label] = pd.read_csv(file, index_col=0)\n",
    "\n",
    "    # Column names are strings, convert to ints\n",
    "    all_imfs[label].columns = all_imfs[label].columns.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2b23c2b2505167",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the IMFs\n",
    "from pySPADS.visualisation.imfs import plot_imfs\n",
    "\n",
    "imf_plot_folder = folder / \"figures\" / \"imfs\"\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "for label, imf_df in tqdm(all_imfs.items(), desc=\"Plotting IMFs\"):\n",
    "    plot_imfs(imf_df.to_numpy().T, label, imf_plot_folder / f\"{label}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c2bf1a6e65450a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Reject components which are primarily noise\n",
    "# Note that this might not be valid when applied to the result of CEEMDAN (rather than EMD), so it should be optional\n",
    "from PyEMD.checks import whitenoise_check\n",
    "\n",
    "for label, imf_df in all_imfs.items():\n",
    "    print(f\"Checking {label}\")\n",
    "    sig = whitenoise_check(imf_df.to_numpy().T)  # , test_name='apriori')\n",
    "    rejects = [k for k, v in sig.items() if v == 0]\n",
    "    print(f\"Rejecting: {rejects}\")\n",
    "    all_imfs[label] = imf_df.drop(columns=[i - 1 for i in rejects])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d066e976c308a1e8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of components in each signal\n",
    "print(\"Number of component IMFs in each signal:\")\n",
    "for label, imf_df in all_imfs.items():\n",
    "    print(f\"{label:>12}: {imf_df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db8d9ef34422366",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Match up components in input/output by frequency\n",
    "from pySPADS.processing.recomposition import component_frequencies\n",
    "\n",
    "# Get the maxima of each component\n",
    "max_imfs = max([imf_df.shape[1] for imf_df in all_imfs.values()])\n",
    "freq_df = pd.DataFrame(columns=all_imfs.keys(), index=list(range(max_imfs)))\n",
    "for label, imf_df in all_imfs.items():\n",
    "    freq_df[label] = component_frequencies(imf_df)\n",
    "\n",
    "freq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8926ead5ed15f9ca",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig2 = paper.fig2(shore_df[\"y\"], all_imfs[\"output\"], \"1999-01-01\", \"2017-01-01\")\n",
    "fig2.savefig(folder / \"figures\" / \"fig2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bdef4be19352a6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig3 = paper.fig3(all_imfs, \"output\", \"1999-01-01\", \"2017-01-01\")\n",
    "fig3.savefig(folder / \"figures\" / \"fig3.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e005c55de5a6f534",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_cols = set(freq_df.columns) - {\"output\"}\n",
    "output_index = all_imfs[\"output\"].columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19052c290966f7da",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find the closest match for each component in output\n",
    "from pySPADS.processing.recomposition import nearest_frequency\n",
    "\n",
    "nearest_freq = nearest_frequency(freq_df[\"output\"], freq_df.drop(columns=[\"output\"]))\n",
    "\n",
    "nearest_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2175a2e3d41149",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check to see if difference in frequencyes is within tolerance\n",
    "from pySPADS.processing.recomposition import frequency_difference\n",
    "\n",
    "diff_df = frequency_difference(\n",
    "    freq_df[\"output\"], freq_df.drop(columns=[\"output\"]), nearest_freq\n",
    ")\n",
    "diff_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a56d017e283b67",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pySPADS.processing.recomposition import relative_frequency_difference\n",
    "\n",
    "rel_diff_df = relative_frequency_difference(\n",
    "    freq_df[\"output\"], freq_df.drop(columns=[\"output\"]), nearest_freq\n",
    ")\n",
    "rel_diff_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ad5cfe1b979d10",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tolerance = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda87c917df7bef8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check that each output component has some valid inputs\n",
    "valid_components = (rel_diff_df < tolerance).sum(axis=1)\n",
    "if any(valid_components == 0):\n",
    "    raise ValueError(\n",
    "        f\"No valid input components for output components: {valid_components[valid_components == 0].index}\"\n",
    "    )\n",
    "\n",
    "if any(valid_components < 3):\n",
    "    print(\n",
    "        f\"Warning: some output components have less than 3 valid input components: {valid_components[valid_components < 3].index}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d4ce6ee9dddfe9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rel_diff_df < tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0161ebb7763869b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show the components which are used for each output component\n",
    "print(\"Components used for each output component:\")\n",
    "for i in output_index:\n",
    "    print(f\"{i:>5} : {np.sum(rel_diff_df.loc[i] < tolerance)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e87c41e139abe06",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO - set nearest_freq to NaN for components which are not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117e6cc1efb26c34",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO - note below - we have more output data than input, we should have truncated the output data before fitting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906b5b86fcec2be",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Linear regression of components to output signal\n",
    "# Note that this is much simpler if the original data interpolated over NaNs\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from pySPADS.optimization import MReg2\n",
    "\n",
    "# TODO - note below - we have more output data than input, we should have truncated the output data before fitting!\n",
    "hindcast_index = all_imfs[\"output\"].index[\n",
    "    all_imfs[\"output\"].index.isin(all_imfs[\"PC0\"].index)\n",
    "]\n",
    "\n",
    "imf_predictions = pd.DataFrame(index=hindcast_index, columns=output_index)\n",
    "mreg_predictions = pd.DataFrame(index=hindcast_index, columns=output_index)\n",
    "\n",
    "for i, imf in enumerate(output_index):\n",
    "    print(f\"Fitting component {imf}\")\n",
    "    X = pd.DataFrame(index=hindcast_index)\n",
    "    for label in input_cols:\n",
    "        if rel_diff_df.loc[imf, label] < tolerance:\n",
    "            # X[label] = all_imfs[label].iloc[:, nearest_freq.loc[imf, label]]\n",
    "            X[label] = all_imfs[label].loc[\n",
    "                all_imfs[label].index.isin(hindcast_index), nearest_freq.loc[imf, label]\n",
    "            ]\n",
    "    y = all_imfs[\"output\"].loc[hindcast_index, imf]\n",
    "    reg = LinearRegression().fit(X, y)\n",
    "    print(f\"R^2: {reg.score(X, y)}\")\n",
    "    print(f\"Coefficients: {reg.coef_}\")\n",
    "    mreg2 = MReg2().fit(X, y)\n",
    "    coefs = mreg2.coef_\n",
    "    print(coefs)\n",
    "    print(f\"Intercept: {reg.intercept_}\")\n",
    "    # Plot the fit\n",
    "    plt.figure()\n",
    "    plt.plot(y, label=\"output\")\n",
    "    plt.plot(y.index, reg.predict(X), label=\"fit\")\n",
    "    mreg_pred = np.sum([X[c] * coefs[i] for i, c in enumerate(X.columns)], axis=0)\n",
    "    plt.plot(y.index, mreg_pred, label=\"mreg\")\n",
    "    imf_predictions.loc[:, imf] = reg.predict(X)\n",
    "    mreg_predictions.loc[:, imf] = mreg_pred\n",
    "    plt.legend()\n",
    "    plt.title(f\"Component {imf}\")\n",
    "    plt.show()\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a5a52828f42dc8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d57ff4a2ed070af",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mreg_pred = np.sum([X[c] * coefs[i] for i, c in enumerate(X.columns)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13e72dba8563a0f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_imfs[\"output\"].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46463a5715cbc599",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imf_predictions.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7054df7f1d5dc84a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig4 = paper.fig4(\n",
    "    shore_df[\"y\"], imf_predictions.sum(axis=1), \"2010-01-01\", \"2017-01-01\"\n",
    ")\n",
    "\n",
    "fig4.savefig(folder / \"figures\" / \"fig4.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba2f7d352b0cbee",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig4a = paper.fig4(\n",
    "    shore_df[\"y\"], mreg_predictions.sum(axis=1), \"2010-01-01\", \"2017-01-01\"\n",
    ")\n",
    "\n",
    "fig4a.savefig(folder / \"figures\" / \"fig4a.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb310c682f4776d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pySPADS.processing.bridge import datenum_to_datetime\n",
    "\n",
    "# TODO - note that above covers 1998-, paper uses only 2010- data\n",
    "datenum_to_datetime(737000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02894060b6a5115",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "#   repeat the process for other noise values (and average?)\n",
    "#   tweak tolerance, noises, other settings?\n",
    "#   check regression implementation\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
